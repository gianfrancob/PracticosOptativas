{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5d6b13d-10a1-45be-a470-81a3e8a0f6ee"
    }
   },
   "source": [
    "# Introducción al Aprendizaje por Refuerzos\n",
    "\n",
    "### Curso Aprendizaje por Refuerzos, Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\n",
    "\n",
    "### FaMAF, 2019\n",
    "\n",
    "#### Agenda\n",
    "\n",
    "* Presentación Docentes\n",
    "* Introducción. Diferencias entre Aprendizaje Supervisado, No Supervisado y Por Refuerzos\n",
    "* Modelo Agente Entorno. Agente Situado. Arquitectura Actor-Crítico.\n",
    "* Aprendizaje por Refuerzos. Elementos. Ciclo del Aprendizaje por Refuerzos. Definición Formal.\n",
    "* Procesos de Decisión de Markov. Función de Valor. Ecuación de Bellman. Optimalidad.\n",
    "* Diferencias Temporales\n",
    "* SARSA y Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docentes\n",
    "\n",
    "* Jorge A. Palombarini\n",
    "* Juan Cruz Barsce\n",
    "* Ezequiel Beccaría\n",
    "\n",
    "## Página y Libro Sutton\n",
    "\n",
    "http://incompleteideas.net/\n",
    "\n",
    "https://drive.google.com/file/d/1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción: Diferencias entre Aprendizaje Supervisado, No Supervisado y Por Refuerzos\n",
    "\n",
    "![](images/Aprendizaje.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entidad Inteligente -> Agente Situado\n",
    "\n",
    "* La idea de aprender por interacción con nuestro entorno es quizás la primera en aparecer cuando pensamos acerca de la naturaleza del aprendizaje.\n",
    "\n",
    "* Por ejemplo, cuando un niño juega, mueve sus brazos, o mira alrededor, no tiene un \"maestro\" explicito, pero posee una conexion sensorial y motora con su entorno.\n",
    "\n",
    "* El ejercicio de dicha conexión produce información acerca de la relación causa-efecto y las consecuencias de las acciones que lleva a cabo, y respecto de qué hacer de manera tal de lograr objetivos. Así, *aprender por interacción* es una idea fundacional de muchas teorías del aprendizaje y la inteligencia.\n",
    "\n",
    "*  En este curso, se explorará un **enfoque computacional dirigido por objetivos para *aprender por interacción***: el **Aprendizaje por Refuerzos (Reinforcement Learning)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent-Environment Framework\n",
    "\n",
    "* El desarrollo de la inteligencia requiere que la entidad o el agente esté situada/o en un entorno **(Measuring universal intelligence: Towards an anytime intelligence test, Hernandez-Orallo & Dowe, Artificial Intelligence, 2010).**\n",
    "![](images/Situated_Agent.png)\n",
    "\n",
    "* El agente y su entorno interactúan a través de la ejecución de acciones, observación de estados y señales de reward. La inteligencia tendrá efecto sólo si el agente tiene claramente definidos objetivos o metas que persigue activamente mientras ocurre la interacción.\n",
    "![](images/AE_Interaction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por Refuerzos (Reinforcement Learning)\n",
    "\n",
    "* **Reinforcement learning** consiste en aprender **qué hacer** -mapear situaciones a acciones- de manera tal de **maximizar** una señal numérica de recompensa. \n",
    "\n",
    "* Al aprendiz (**agente**) no se le especifica cuáles acciones ejecutar, sino que debe descubrir a través de ***prueba y error*** cuáles acciones producen la mayor cantidad de recompensa acumulada. En los casos más interesantes, las acciones pueden afectar no solo la recompensa inmediata, sino solamente el estado, recibiéndose una recompensa sólo en algunos estados o bien al final del episodio (***Delayed-reward***).\n",
    "\n",
    "* Se conoce simultaneamente como RL al problema de **aprender por interacción**, a las **métodos de solución** de dicho problema, y al **área de la IA que estudia el problema y los métodos de solución**.\n",
    "\n",
    "* El problema de RL puede ser formalizado empleando **Procesos de Decisión de Markov**.\n",
    "\n",
    "* La toma de decisiones secuencial involucra aprender sobre nuestro entorno y elegir acciones que maximizan el retorno esperado. El RL computacional, inspirado por estas ideas, las formalizo y produjo un impacto importante en robótica, machine learning y neurociencias.\n",
    "\n",
    "> El Aprendizaje por Refuerzos (RL) consiste en un agente que se encuentra en algún estado $s \\in S$ inmerso en un entorno $E$ y ejecuta acciones $a \\in A$ en busca de una meta. El agente puede ser modelado formalmente como una función f, que toma un  historial de interacción como entrada, y devuelve una acción a tomar. Una manera conveniente para representar el agente es una medida de probabilidad sobre el set A de acciones, en base a un historial de interacción: $$ f(a_{n}|s_{1}a_{1} r_{2} s_{2}a_{2}...r_{n}s_{n}) $$ que representa la probabilidad de la acción a en el ciclo n dado un historial de interacción.\n",
    "\n",
    "![](images/fox-rl.png)\n",
    "\n",
    "* Problema RL: ***¿Cómo el agente produce la distribución de probabilidad sobre las acciones?***\n",
    "\n",
    "* ***Dilema de exploración - explotación***: debido a que el Agente no recibe ejemplos de entrenamiento, debe probar alternativas, procesar los resultados de sus acciones y modificar su comportamiento en algún sentido. ¿Cuándo explotar este conocimiento vs. cuándo probar nuevas estrategias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitectura Actor-Crítico\n",
    "![](images/RL_Diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementos del Aprendizaje por Refuerzos\n",
    "![](images/RL_Elements.png)\n",
    "\n",
    "* **Policy (Política): **\n",
    "\n",
    "Una política define la manera de comportarse de un agente, en cualquier momento de tiempo dado. Basicamente, es un mapeo de un estado o percepción s a una acción a, pudiendo ser estocásticas.\n",
    "\n",
    "* **Reward Function (Función de Recompensa)**\n",
    "\n",
    "Define cuantitativamente el objetivo del agente. Es un mapeo de un par estado-acción a un número real que indica \"cuán deseable\" es ejecutar dicha acción en ese estado. Asimismo, el único objetivo del agente es maximizar la recompensa total que recibe a lo large del tiempo. Cabe mencionar que, si bien la función de reward no puede ser alterada por el agente, provee las bases para cambiar la política del mismo.\n",
    "\n",
    "* **Value function (Función de Valor)**\n",
    "\n",
    "La función de valor se diferencia de la función de reward en el sentido de que indica \"cuán deseable\" es, a largo plazo, ejecutar una acción en un determinado estado. Así, el valor de un estado *s* es la cantidad total de reward que el agente espera obtener a futuro comenzando la interacción en el estado *s*.\n",
    "\n",
    "* **Environment (Entorno)**\n",
    "\n",
    "El entorno se encuentra constitutido por todo aquel elemento (real o simulado) que el agente no puede controlar. Es con quién el agente interactúa a partir de la ejecución de acciones de control.\n",
    "\n",
    "## Ciclo del Aprendizaje por Refuerzos\n",
    "![](images/RL_Cycle.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "84a1b72e-5d2e-4343-a014-0fcfee9373e4"
    }
   },
   "source": [
    "### Definición formal\n",
    "\n",
    "* Si el problema de RL dado tiene un conjunto finito de estados y acciones y satisface la propiedad de Markov entonces puede definirse como un Proceso de Decisión de Markov\n",
    "\n",
    "\\begin{equation}\n",
    "MDPFinito = (S, A, P(.), R(.), γ)\n",
    "\\end{equation}\n",
    "\n",
    "donde\n",
    "\n",
    "$$ S = {s_{1}, s_{2}, ..., s_{n}} $$\n",
    "es un conjunto finito de estados.\n",
    "\n",
    "![](images/ajedrez-estado.jpg)\n",
    "\n",
    "![](images/doom-estado.png)\n",
    "\n",
    "$$ A = {a_{1}, a_{2}, ..., a_{m}} $$\n",
    "es un conjunto finito de acciones.\n",
    "$$ P_{a}(s,s') = P(s_{t+1} = s | s_{t} = s, a_{t} = a) $$ \n",
    "es la probabilidad de que la acción a tomada en tiempo t y en estado s lleve al agente al estado s' en tiempo t+1\n",
    "$$ R_{a}(s,s') $$\n",
    "es la recompensa inmediata recibido tras transicionar, luego de tomar la acción a, desde el estado s al estado s'\n",
    "$$\\gamma \\in  [0,1]$$ \n",
    "es el factor de descuento, representando la diferencia en la importancia de la recompensa a corto plazo vs la recompensa a largo plazo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mdp_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "197f95d9-38cd-4f22-830a-1c484037b895"
    }
   },
   "source": [
    "* Un **episodio** (instancia) de este MDP forma una secuencia finita \n",
    "$$s_{0}, a_{0}, r_{1}, s_{1}, a_{1}, r_{2}, s_{2}, ... , s_{n-1}, a_{n-1}, r_{n}, s_{n} $$ \n",
    "donde $$s_{n}$$\n",
    "es un estado final (o n es el tiempo de corte).\n",
    "\n",
    "* La recompensa total del episodio está dado por \n",
    "$$ R = r_{1} + r_{2} + ... + r_{n} $$\n",
    "\n",
    "* En consecuencia, la recompensa a futuro partiendo del tiempo $t$ está dado por \n",
    "$$R_{t} = r_{t} + r_{t+1} + ... $$\n",
    "\n",
    "* Hay que considerar que el ambiente es estocástico en la mayor parte de los entornos reales y, por tanto, la recompensa suele diverger mientras más alejado se encuentre el instante de tiempo considerado. Es por esto que se utiliza un parámetro $γ$ llamado _factor de descuento_, para descontar el valor de las recompensas futuras. De esta manera,\n",
    "\n",
    "\\begin{equation} R_{t} = r_{t} + γr_{t+1} + γ^2r_{t+2} + γ^3r_{t+3} + ... = r_{t} + γ(r_{t+1} + γr_{t+2} + γ^2r_{t+3} ...) = r_{t} + γR_{t+1} \\end{equation}\n",
    "\n",
    "* Si utilizamos $γ=0$, el agente priorizará sólo la recompensa inmediata, mientras que $\\gamma=1$ hará que considere todas los recompensas de la misma manera, independientemente del momento en donde las reciba.\n",
    "\n",
    "![](images/RL_Problem_Statement.png)\n",
    "\n",
    "* Dado un Proceso de Decisión de Markov, una Política (determinística) es una función $\\pi$ que a partir de un estado $s \\in S$, devuelve como resultado una acción $a \\in A$. Una política estocástica devuelve, para un estado $s$ y una acción $a$, una probabilidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesos de Decisión de Markov\n",
    "\n",
    "**Resolver un problema de decisión modelado como un PDM implica encontrar los valores de la función de valor V**\n",
    "\n",
    "### Función de Valor\n",
    "\n",
    "* El valor de un estado es el retorno esperado por el agente, comenzando la interacción en dicho estado, dependiendo de la política ejecutada por el agente.\n",
    "\n",
    "![](images/Funcion_de_Estado_Valor.png)\n",
    "\n",
    "* El valor de la ejecución de una acción en un estado es el retorno esperado por el agente, comenzando la interacción en dicho estado a partir de la ejecución de dicha acción, dependiendo de la política ejecutada por el agente.\n",
    "\n",
    "![](images/Funcion_de_Accion_Valor.png)\n",
    "\n",
    "Una propiedad fundamental de las funciones de valor es que satisfacen ciertas propiedades recursivas. Para cualquier política  π y cualquier estado s, V(s) y Q(s,a) pueden ser definidas recursivamente en términos de la denominada *Ecuación de Bellman* ** (Bellman, 1957) **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuación de Bellman\n",
    "\n",
    "* La idea básica es:\n",
    "\n",
    "![](images/Retorno.png)\n",
    "\n",
    "* Entonces,\n",
    "\n",
    "![](images/Retorno_Valor.png)\n",
    "\n",
    "* O, sin el operador de valor esperado:\n",
    "\n",
    "![](images/Bellman_Equation.png)\n",
    "\n",
    "La ecuación anterior refleja el hecho de que el valor de un estado se encuentra definido en términos de la recompensa inmediata y los valores de los estados siguientes ponderados en función de las probabilidades de transición, y adicionalmente un factor de descuento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuación de Optimalidad de Bellman\n",
    "\n",
    "La Ecuación de Optimalidad de Bellman refleja el hecho de que el Valor de un estado bajo la política óptima debe ser igual al retorno esperado para la mejor acción en dicho estado:\n",
    "\n",
    "![](images/Ecuacion_de_Optimalidad_Valor.png)\n",
    "\n",
    "Al mismo tiempo, la acción óptima para un estado s dada la función de valor, puede obtenerse mediante:\n",
    "\n",
    "![](images/Accion_Optima.png)\n",
    "\n",
    "La política anterior se denomina **Política Greedy**, dado que selecciona la mejor acción para cada estado, teniendo en cuenta la función de valor V(s). De manera análoga, la función de acción-valor óptima puede expresarse como:\n",
    "\n",
    "![](images/Accion_Valor_Optima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ComparacionPoliticas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo MDP y Políticas (*)\n",
    "\n",
    "![](images/GrafoPolitica.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/GrafoPolitica1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/GrafoPolitica2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/GrafoPolitica3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/GrafoPolitica4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/GrafoPolitica5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\(*) entorno de ejemplo basado en curso de Aprendizaje Profundo por Refuerzos, dictado por Juan Gómez Romero en la Escuela de Ciencias Informáticas 2019 - UBA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1\n",
    "\n",
    "[RL Warm Up Lab!](lab_0_warmup_rl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximaciones para el aprendizaje de V y Q mediante Soluciones Tabulares\n",
    "\n",
    "![](images/Aproximaciones al aprendizaje.png)\n",
    "\n",
    "### Model Based vs. Model Free\n",
    "\n",
    "* Model-free aprende Q/V directamente y presenta muy baja complejidad computacional.\n",
    "\n",
    "* Model-based aprende T y R y usa un algoritmo de planning para encontrar la política. Uso eficiente de los datos/experiencia. Alto costo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aprendizaje por Diferencias Temporales (TD Learning).\n",
    "\n",
    "* La idea principal es actualizar una predicción de la función de valor en base al cambio que existe en la misma de un momento al siguiente (**Diferencia Temporal o Temporal Difference**)\n",
    "![](images/Diferencia_Temporal.png)\n",
    "\n",
    "* Los algoritmos de aprendizaje basados en TD se emplean en mayor medida para realizar el CONTROL respecto de las acciones que ejecuta un agente que interactúa con su entorno. De esa manera, en lugar de aprender la función de estado-valor **V**, se orientan al aprendizaje de la función de acción-valor **Q**.\n",
    "\n",
    "* En particular, existen dos enfoques principales para realizar el aprendizaje de funciones **Q**, ambos considerando el trade-off exploración/explotación: **on-policy** y **off-policy**. En particular, los métodos **on-policy** estiman $Q_{π}(s, a)$ para la política π que el agente se encuentra ejecutando, para todos los estados s y acciones a. \n",
    "\n",
    "* Dicha estimación puede ser realizada empleando el mismo método TD descripto anteriormente para actualizar $V_{π}$, an base a:\n",
    "\n",
    "![](images/Actualizacion_QEstadoAccion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade-off exploración explotación\n",
    "\n",
    "* Es el dilema en que incurre el agente al momento de **elegir entre distintas acciones**.\n",
    "\n",
    "* Consiste en optar entre elegir una acción (a priori óptima) cuyos efectos son conocidos, esperando obtener un resultado similar (_explotar_) o elegir otra (posiblemente no óptima) cuyos efectos son desconocidos, pero que puede conducir a aprender más (_explorar_)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA: On-Policy TD Learning\n",
    "\n",
    "* El agente y su entorno interactúan a través de la ejecución de acciones, observación de estados y señales rewards. La inteligencia tendrá efecto sólo si el agente tiene claramente definidos objetivos o metas que persigue activamente mientras ocurre la interacción.\n",
    "\n",
    "![](images/SARSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 2\n",
    "\n",
    "[Lab Introducción al RL](lab_1_intro_rl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "* Bootstrapping in RL puede entenderse como \"emplear uno o más valores estimados en la actualización del mismo tipo de valor estimado\".\n",
    "\n",
    "* En el caso de los métodos TD:\n",
    "\n",
    "![](images/Bootstrapping.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Presenta la desventaja de estar sesgado hacia los valores iniciales de la función de valor Q o V (Problema importante si empleamos estimadores, el sistema se autorreferencia)\n",
    "\n",
    "* Empleando trayectorias más largas (Métodos MC) tenemos el problema de la alta varianza -> Necesitamos mayor cantidad de ejemplos para converger\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning: Off-Policy TD Learning\n",
    "\n",
    "* Uno de los más importantes avances en RL fue el desarrollo del algoritmo **off-policy** conocido como Q-learning (Watkins, 1989). En su forma más simple, en Q-learning la actualización de la función de acción-valor realizada se define por:\n",
    "\n",
    "![](images/QLearningUpdate.png)\n",
    "\n",
    "* Es este caso, la función de acción-valor Q aproxima directamente q∗, es decir, la función de acción-valor correspondiente a la política óptima, independientemente de la política seguida por el agente (de ahí su clasificación como **off-policy**), la cuál tiene efecto en el proceso de selección de acciones y por lo tanto determina cuales pares estado-acción se actualizan.\n",
    "\n",
    "![](images/QLearning.png)\n",
    "\n",
    "## Ejemplo On-Policy vs. Off-Policy: The Cliff\n",
    "\n",
    "![](images/onpolicyvsoffpolicy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbpresent": {
   "slides": {
    "0d01fc30-23a9-4dc7-a99a-309a23aa7a9b": {
     "id": "0d01fc30-23a9-4dc7-a99a-309a23aa7a9b",
     "prev": "899c419b-b69d-4a42-8c8f-ec9e170f2953",
     "regions": {
      "d7b7ed9a-ebe1-45b7-b520-ee8111741f6f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4951a012-93f0-4561-b2aa-f356d9fd10af",
        "part": "whole"
       },
       "id": "d7b7ed9a-ebe1-45b7-b520-ee8111741f6f"
      }
     }
    },
    "1265d14e-4373-4961-b094-fdc0f41bd665": {
     "id": "1265d14e-4373-4961-b094-fdc0f41bd665",
     "prev": "69ef8fab-1cf4-4791-8359-fbd8c36b87f5",
     "regions": {
      "60a1d8af-4d7e-49aa-923c-5bc0d4914c2c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c7d530e5-f080-4f70-9482-c4612911cbe3",
        "part": "whole"
       },
       "id": "60a1d8af-4d7e-49aa-923c-5bc0d4914c2c"
      }
     }
    },
    "1315de08-3be9-4fc0-81e0-acc069ac5044": {
     "id": "1315de08-3be9-4fc0-81e0-acc069ac5044",
     "prev": null,
     "regions": {
      "b47d0e6d-4dcb-4a20-8e5e-874847ad2b76": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d5d6b13d-10a1-45be-a470-81a3e8a0f6ee",
        "part": "whole"
       },
       "id": "b47d0e6d-4dcb-4a20-8e5e-874847ad2b76"
      }
     }
    },
    "1809123f-e0c6-4859-bc21-f3b5dc441a1b": {
     "id": "1809123f-e0c6-4859-bc21-f3b5dc441a1b",
     "prev": "3ad98c91-7a86-4a2a-9ecf-cfc6db0d3916",
     "regions": {
      "9c137fb8-1b4f-427e-9b34-49632cbebe6d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "037b358e-6d28-4df4-ae9d-974f8775e00e",
        "part": "whole"
       },
       "id": "9c137fb8-1b4f-427e-9b34-49632cbebe6d"
      }
     }
    },
    "1c00d850-e4dc-4916-bbe5-b275c71d185b": {
     "id": "1c00d850-e4dc-4916-bbe5-b275c71d185b",
     "prev": "982cfab9-3c43-45c9-9733-6e3c27538085",
     "regions": {
      "27107d62-642d-48d1-96c8-f9b6e21001d9": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9da47f7b-28d6-44cb-bac2-daae14fab987",
        "part": "whole"
       },
       "id": "27107d62-642d-48d1-96c8-f9b6e21001d9"
      }
     }
    },
    "223fdf1f-941d-4667-819b-4082691bb812": {
     "id": "223fdf1f-941d-4667-819b-4082691bb812",
     "prev": "6afb007f-1120-46a8-8c15-2cdaa7286dab",
     "regions": {
      "3f59bb53-c37b-4f27-bc1e-1ab27bc75d99": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "58ec6acb-8f4f-4fda-af22-9f4703e72336",
        "part": "whole"
       },
       "id": "3f59bb53-c37b-4f27-bc1e-1ab27bc75d99"
      }
     }
    },
    "313101ba-a6ea-4985-b56a-7e5faeb4ef11": {
     "id": "313101ba-a6ea-4985-b56a-7e5faeb4ef11",
     "prev": "90bfd2ff-9a21-4de9-8584-dd17db513b9b",
     "regions": {
      "00901c26-d4cf-4a49-8a0e-4e61005f043c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "2be788a6-2235-491e-aab5-0bb567fe525d",
        "part": "whole"
       },
       "id": "00901c26-d4cf-4a49-8a0e-4e61005f043c"
      }
     }
    },
    "3194ed44-3f5e-49b0-bdb0-38553756e922": {
     "id": "3194ed44-3f5e-49b0-bdb0-38553756e922",
     "prev": "f6cecdf6-a875-43e5-8544-93b4f9abb66d",
     "regions": {
      "541d2ce3-01f2-49f4-a6d1-f41765d0a18c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ef536408-da22-4ef2-808e-2a65e7081661",
        "part": "whole"
       },
       "id": "541d2ce3-01f2-49f4-a6d1-f41765d0a18c"
      }
     }
    },
    "3ad98c91-7a86-4a2a-9ecf-cfc6db0d3916": {
     "id": "3ad98c91-7a86-4a2a-9ecf-cfc6db0d3916",
     "prev": "d6d405a0-dfde-4a37-a754-7db5e780e0af",
     "regions": {
      "a200fdd8-1279-4bbb-9cba-eac12cf5dbcb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c4915b91-8222-4388-b0bc-3bb6a4d87b54",
        "part": "whole"
       },
       "id": "a200fdd8-1279-4bbb-9cba-eac12cf5dbcb"
      }
     }
    },
    "3ea64685-7182-46a8-b427-641f90be1dc9": {
     "id": "3ea64685-7182-46a8-b427-641f90be1dc9",
     "prev": "dcf0e1b4-2b46-4603-bb28-5f8e80ffd702",
     "regions": {
      "d14104e3-1e91-4507-be35-db71a6dd41a8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "eb46d21c-4caa-42a9-a197-4d64c603fd9b",
        "part": "whole"
       },
       "id": "d14104e3-1e91-4507-be35-db71a6dd41a8"
      }
     }
    },
    "4257ba12-3eec-48b3-8dc4-a92664887a5d": {
     "id": "4257ba12-3eec-48b3-8dc4-a92664887a5d",
     "prev": "92e07c6c-11e0-4795-bda1-abe9ddfd4d02",
     "regions": {
      "efaedb93-d56a-491b-8660-b8e2e5b07519": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9bc1fb76-8a78-4f38-a718-38d1e15cc1b3",
        "part": "whole"
       },
       "id": "efaedb93-d56a-491b-8660-b8e2e5b07519"
      }
     }
    },
    "4f4a9683-905f-4595-9760-d56a847ae47c": {
     "id": "4f4a9683-905f-4595-9760-d56a847ae47c",
     "prev": "4257ba12-3eec-48b3-8dc4-a92664887a5d",
     "regions": {
      "c00eb85e-4e9e-48ac-8119-2a14a98c2042": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6bee5ed5-2110-428b-9f5b-feb464858aaf",
        "part": "whole"
       },
       "id": "c00eb85e-4e9e-48ac-8119-2a14a98c2042"
      }
     }
    },
    "69ef8fab-1cf4-4791-8359-fbd8c36b87f5": {
     "id": "69ef8fab-1cf4-4791-8359-fbd8c36b87f5",
     "prev": "223fdf1f-941d-4667-819b-4082691bb812",
     "regions": {
      "58b36bb4-8aa5-44e0-b88d-db85a5ad1e5c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a61ee6dd-76b9-4ed4-93e7-e28a25ae4490",
        "part": "whole"
       },
       "id": "58b36bb4-8aa5-44e0-b88d-db85a5ad1e5c"
      }
     }
    },
    "6afb007f-1120-46a8-8c15-2cdaa7286dab": {
     "id": "6afb007f-1120-46a8-8c15-2cdaa7286dab",
     "prev": "dd511c07-9814-43e5-b7c0-31151b3582e9",
     "regions": {
      "d3c11af7-2f8c-4b6f-af02-8e5ab8a114db": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e118cd6c-e993-4a82-a052-caeb13fb8cad",
        "part": "whole"
       },
       "id": "d3c11af7-2f8c-4b6f-af02-8e5ab8a114db"
      }
     }
    },
    "6c974a58-be8c-42b2-a4e5-e31607f31727": {
     "id": "6c974a58-be8c-42b2-a4e5-e31607f31727",
     "prev": "1c00d850-e4dc-4916-bbe5-b275c71d185b",
     "regions": {
      "1b3a3c08-25e5-40a5-8faf-2855fea2e3ac": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f417f2ff-615d-4adc-bb45-0482f34e77c0",
        "part": "whole"
       },
       "id": "1b3a3c08-25e5-40a5-8faf-2855fea2e3ac"
      }
     }
    },
    "7d5b2ad9-38cb-4ce1-8c24-7774740b8710": {
     "id": "7d5b2ad9-38cb-4ce1-8c24-7774740b8710",
     "prev": "cef2e9fb-fb0c-4d2c-9264-fa08d3de7788",
     "regions": {
      "17c4f751-c034-4d71-9d8b-eaa8bad1602f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "197f95d9-38cd-4f22-830a-1c484037b895",
        "part": "whole"
       },
       "id": "17c4f751-c034-4d71-9d8b-eaa8bad1602f"
      }
     }
    },
    "899c419b-b69d-4a42-8c8f-ec9e170f2953": {
     "id": "899c419b-b69d-4a42-8c8f-ec9e170f2953",
     "prev": "6c974a58-be8c-42b2-a4e5-e31607f31727",
     "regions": {
      "0d0ff77c-47e7-4ecf-8542-348d395aa416": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "96467e64-3610-4e91-8034-b35e87ac68fb",
        "part": "whole"
       },
       "id": "0d0ff77c-47e7-4ecf-8542-348d395aa416"
      }
     }
    },
    "8ee0867a-0485-4903-9e16-0b1a83bad931": {
     "id": "8ee0867a-0485-4903-9e16-0b1a83bad931",
     "prev": "9b077531-fbbf-4d74-8bd4-55550025f933",
     "regions": {
      "26cefc3d-e27f-46ec-be1d-e8c0849eb199": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9248f5e9-f98c-458a-ae3a-71512b071a5b",
        "part": "whole"
       },
       "id": "26cefc3d-e27f-46ec-be1d-e8c0849eb199"
      }
     }
    },
    "90bfd2ff-9a21-4de9-8584-dd17db513b9b": {
     "id": "90bfd2ff-9a21-4de9-8584-dd17db513b9b",
     "prev": "adb231cc-0652-40e4-bc3c-9ce52082cb6b",
     "regions": {
      "83eddba8-c706-49a3-89bd-b510bc2c5008": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6bdd9f90-0795-44d8-a18d-c13301520398",
        "part": "whole"
       },
       "id": "83eddba8-c706-49a3-89bd-b510bc2c5008"
      }
     }
    },
    "90defec3-5b4d-470f-a68d-6a6fae9ebbec": {
     "id": "90defec3-5b4d-470f-a68d-6a6fae9ebbec",
     "prev": "8ee0867a-0485-4903-9e16-0b1a83bad931",
     "regions": {
      "fe8179b7-977e-48dc-be62-abf41f233a62": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1d808ce4-f5b3-4c18-818d-04c00ee0d322",
        "part": "whole"
       },
       "id": "fe8179b7-977e-48dc-be62-abf41f233a62"
      }
     }
    },
    "92e07c6c-11e0-4795-bda1-abe9ddfd4d02": {
     "id": "92e07c6c-11e0-4795-bda1-abe9ddfd4d02",
     "prev": "0d01fc30-23a9-4dc7-a99a-309a23aa7a9b",
     "regions": {
      "fb6e6866-7208-41e1-b6ba-9450762a1241": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a3de943f-ece8-4dc5-8221-6611d4b59541",
        "part": "whole"
       },
       "id": "fb6e6866-7208-41e1-b6ba-9450762a1241"
      }
     }
    },
    "982cfab9-3c43-45c9-9733-6e3c27538085": {
     "id": "982cfab9-3c43-45c9-9733-6e3c27538085",
     "prev": "313101ba-a6ea-4985-b56a-7e5faeb4ef11",
     "regions": {
      "daed9365-6bd9-4bcb-a676-208bb12b4361": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4c8e853f-b925-4f45-82f2-36069d4f094b",
        "part": "whole"
       },
       "id": "daed9365-6bd9-4bcb-a676-208bb12b4361"
      }
     }
    },
    "9b077531-fbbf-4d74-8bd4-55550025f933": {
     "id": "9b077531-fbbf-4d74-8bd4-55550025f933",
     "prev": "1809123f-e0c6-4859-bc21-f3b5dc441a1b",
     "regions": {
      "3ec109fc-1ac8-4762-847e-5e6465f9cae8": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "570ee83e-6db4-4a0e-b4ca-76c9665199d7",
        "part": "whole"
       },
       "id": "3ec109fc-1ac8-4762-847e-5e6465f9cae8"
      }
     }
    },
    "adb231cc-0652-40e4-bc3c-9ce52082cb6b": {
     "id": "adb231cc-0652-40e4-bc3c-9ce52082cb6b",
     "prev": "3ea64685-7182-46a8-b427-641f90be1dc9",
     "regions": {
      "da3e4bfe-8e1c-4315-bb8d-38def529ab4c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "464da1eb-7686-4f7d-a94f-fd4f4b3ae2e1",
        "part": "whole"
       },
       "id": "da3e4bfe-8e1c-4315-bb8d-38def529ab4c"
      }
     }
    },
    "c4d9cb29-6062-4d67-bacf-309165899909": {
     "id": "c4d9cb29-6062-4d67-bacf-309165899909",
     "prev": "f3ec050f-3d84-4833-87a4-3186460564fb",
     "regions": {
      "4429e8a1-ec45-4949-923e-5e7b08049d2d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8757fda7-751c-4dcd-80c7-fa4097aa62a3",
        "part": "whole"
       },
       "id": "4429e8a1-ec45-4949-923e-5e7b08049d2d"
      }
     }
    },
    "cbf62be1-d44f-43e8-a594-bba802494845": {
     "id": "cbf62be1-d44f-43e8-a594-bba802494845",
     "prev": "c4d9cb29-6062-4d67-bacf-309165899909",
     "regions": {
      "f304038b-504c-46f6-b03c-fc564190e993": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "14bef84c-7b2e-46b7-ac4f-18b9854d609e",
        "part": "whole"
       },
       "id": "f304038b-504c-46f6-b03c-fc564190e993"
      }
     }
    },
    "cef2e9fb-fb0c-4d2c-9264-fa08d3de7788": {
     "id": "cef2e9fb-fb0c-4d2c-9264-fa08d3de7788",
     "prev": "1315de08-3be9-4fc0-81e0-acc069ac5044",
     "regions": {
      "dc60dfcf-1616-4f16-82a0-cb0fade4e740": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "84a1b72e-5d2e-4343-a014-0fcfee9373e4",
        "part": "whole"
       },
       "id": "dc60dfcf-1616-4f16-82a0-cb0fade4e740"
      }
     }
    },
    "d6d405a0-dfde-4a37-a754-7db5e780e0af": {
     "id": "d6d405a0-dfde-4a37-a754-7db5e780e0af",
     "prev": "4f4a9683-905f-4595-9760-d56a847ae47c",
     "regions": {
      "cb634b1b-130b-4ddc-a273-f4d4b766feca": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b0f69589-df64-4288-985f-2cec5d01472f",
        "part": "whole"
       },
       "id": "cb634b1b-130b-4ddc-a273-f4d4b766feca"
      }
     }
    },
    "dcf0e1b4-2b46-4603-bb28-5f8e80ffd702": {
     "id": "dcf0e1b4-2b46-4603-bb28-5f8e80ffd702",
     "prev": "e67834a8-52ef-48d9-90b7-a291acb99e4b",
     "regions": {
      "0789a766-9218-417b-8534-24a6d4772988": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "9d90f76f-96f5-4405-a1f5-4cf4eb5522fd",
        "part": "whole"
       },
       "id": "0789a766-9218-417b-8534-24a6d4772988"
      }
     }
    },
    "dd511c07-9814-43e5-b7c0-31151b3582e9": {
     "id": "dd511c07-9814-43e5-b7c0-31151b3582e9",
     "prev": "eab010b8-7397-4b90-835d-bf2e90bff56c",
     "regions": {
      "bb5d1405-461c-4569-86dd-b21c981872ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "d7e940c4-4812-411a-94d3-7279c2de20cc",
        "part": "whole"
       },
       "id": "bb5d1405-461c-4569-86dd-b21c981872ed"
      }
     }
    },
    "e67834a8-52ef-48d9-90b7-a291acb99e4b": {
     "id": "e67834a8-52ef-48d9-90b7-a291acb99e4b",
     "prev": "cbf62be1-d44f-43e8-a594-bba802494845",
     "regions": {
      "9be8c217-3af4-451c-833e-fc9e910cb804": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ec118c50-e3fc-4dd2-9376-3e598f98167b",
        "part": "whole"
       },
       "id": "9be8c217-3af4-451c-833e-fc9e910cb804"
      }
     }
    },
    "eab010b8-7397-4b90-835d-bf2e90bff56c": {
     "id": "eab010b8-7397-4b90-835d-bf2e90bff56c",
     "prev": "7d5b2ad9-38cb-4ce1-8c24-7774740b8710",
     "regions": {
      "31363f19-2d8a-4bb1-9c82-5125bf553b08": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1b6580a7-ed0d-40c8-b2a7-a413193deae3",
        "part": "whole"
       },
       "id": "31363f19-2d8a-4bb1-9c82-5125bf553b08"
      }
     }
    },
    "f3ec050f-3d84-4833-87a4-3186460564fb": {
     "id": "f3ec050f-3d84-4833-87a4-3186460564fb",
     "prev": "1265d14e-4373-4961-b094-fdc0f41bd665",
     "regions": {
      "9dbd9105-2145-44ca-8fb6-f23ff8438a46": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7770a42f-6089-4f0a-85a9-025f72d3ad5a",
        "part": "whole"
       },
       "id": "9dbd9105-2145-44ca-8fb6-f23ff8438a46"
      }
     }
    },
    "f6cecdf6-a875-43e5-8544-93b4f9abb66d": {
     "id": "f6cecdf6-a875-43e5-8544-93b4f9abb66d",
     "prev": "90defec3-5b4d-470f-a68d-6a6fae9ebbec",
     "regions": {
      "8001bab8-0471-4f0a-9d92-c14df7dfea7d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "945892c8-340a-4bf8-bbcf-1aa29c546187",
        "part": "whole"
       },
       "id": "8001bab8-0471-4f0a-9d92-c14df7dfea7d"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

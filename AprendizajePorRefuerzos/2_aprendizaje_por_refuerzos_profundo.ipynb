{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d5d6b13d-10a1-45be-a470-81a3e8a0f6ee"
    }
   },
   "source": [
    "# Aprendizaje por Refuerzos Profundo\n",
    "\n",
    "### Curso Aprendizaje por Refuerzos, Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones\n",
    "\n",
    "### FaMAF, 2019\n",
    "\n",
    "#### Agenda\n",
    "\n",
    "* Métodos de Solución Aproximada y Generalización en RL\n",
    "* Aproximación Funcional y SGD\n",
    "* Redes Neuronales: Deep Q-networks\n",
    "* Deep Q-learning\n",
    "* Mejoras al Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos de Solución Aproximada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Extensión de los métodos tabulares para aplicarlos a problemas con espacios de estados arbitrariamente grandes (enormes?).\n",
    "* Ej. El número de imágenes posibles de una cámara.\n",
    "* En tales casos, no buscamos una política óptima o una función de valor óptima, sino que generamos **buenas soluciones aproximadas**\n",
    "empleando recursos computacionales limitados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalización en RL\n",
    "\n",
    "El problema de los espacios de estados de gran tamaño no es solo la memoria necesaria para almacenar las tablas de valor, sino el tiempo y los datos necesarios para llenarlas de manera correcta.\n",
    "\n",
    "* En muchos problemas reales casi todos los estados visitados no han sido vistos antes por el agente, por lo que para tomar decisiones en tales estados es necesario llevar a cabo un proceso de **generalización** respecto de la información presente en estados considerados \"similares”.\n",
    "\n",
    "> Pregunta: Cómo se puede generalizar de manera útil la experiencia en base a un número limitado de ejemplos del espacio de estados para producir buenas aproximaciones sobre un set de de estados mucho mayor?\n",
    "\n",
    "* Integrar reinforcement learning con métodos de generalización preexistentes.\n",
    "\n",
    "* Generalización = *function approximation* = **tomar ejemplos de una función y generalizar a partir de ellos para obtener una representación de la función completa. (e.g., función de valor V o Q)**\n",
    "\n",
    "* La aproximación funcional es una instancia del aprendizaje supervisado (artificial neural networks, pattern recognition, and statistical curve fitting). En teoría, cualquiera de los métodos estudiados en dichas áreas podrían emplearse con algoritmos reinforcement learning, aunque en la práctica algunos de ellos son mejores que otros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aproximación Funcional en RL\n",
    "\n",
    "* Uso en la estimación de $V$, $Q$ o $\\pi$.\n",
    "\n",
    "* Las funciones mencionadas no se representan como tablas sino en forma de funciones parametrizadas con un vector de pesos $w \\in \\mathbb{R}^d$\n",
    "* Por ejemplo, $\\hat{v}$ podría ser una función lineal en los features del estado, con $w$ como vector de pesos de los features.\n",
    "* De manera más general, $\\hat{v}$ podría ser la función computada por una red neuronal multicapa, con $w$ representando el vector de pesos de conexiones entre neuronas en todas las capas. Por medio del ajuste de los pesos, un amplio rango de funciones puede ser implementado por la red.\n",
    "* $\\hat{v}$ podría ser la función computada por un árbol de decisión, donde $w$ consiste en los valores que definen las divisiones en las ramas y los valores de las hojas. Normalmente, como la cantidad de pesos (dimensionalidad de $w$) es menor que la cantidad de estados, cambiar un peso cambia la estimacion de valor de muchos estados.\n",
    "* Como consecuencia, cundo un estado se actualiza, el cambio se generaliza desde ese estado para afectar los valores de muchos otros estados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Aproximation_of_v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient-descent (SGD)\n",
    "\n",
    "El vector de pesos es un vector columna con un número fijo de components reales. $w = (w_{1}, w_{2} , . . . , w_{d})$ y la función de valor aproximada $\\hat{v}(s,w)$ es una función diferenciable de w para todo $s \\in S$.\n",
    "\n",
    "* SGD actualiza w en cada uno de los pasos t = 0, 1, 2, 3, . . ., n de interacción intentando minimizar el error de predicción respecto de los ejemplos provenientes de la experimentación con el entorno.\n",
    "\n",
    "* SGD realiza este proceso ajustando el vector de pesos después de la generación de cada ejemplo correcto en una pequeña cantidad en la dirección que más reduciría el error en dicho ejemplo.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_update_w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * **Los métodos basados en SGD emplean el “gradiente descendente” porque el paso de actualización de $w_{t}$ es proporcional al gradiente negativo del error.**\n",
    "    * Son estocásticos porque la actualización se lleva a cabo sobre un ejemplo a la vez, que puede ser seleccionado estocásticamente.\n",
    "    * Variantes: Batch/Minibatch\n",
    "    * http://ruder.io/optimizing-gradient-descent/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate o Step Size\n",
    "\n",
    "![](images/deep_rl_sgd_learning_rate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Function Approximation: Artificial Neural Networks\n",
    "\n",
    "* Las ANNs son ampliamente utilizadas para aproximar funciones no lineales.\n",
    "* Una ANN es una red de unidades interconectadas que tienen algunas de las “propiedades” de las neuronas, los principales componentes de\n",
    "los sistemas nerviosos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_nns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional NNs\n",
    "\n",
    "* Un tipo de ANN empleado en muchas aplicaciones exitosas de RL.\n",
    "* Específicas para el procesamiento de datos de alta dimensionalidad estructurados en arrays, tales como imágenes.\n",
    "\n",
    "![](images/deep_rl_convs_nns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-networks\n",
    "\n",
    "![](images/deep_rl_deep_q_networks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning\n",
    "\n",
    "![](images/deep_rl_bellman_update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_deep_q_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problemas de Deep Reinforcement Learning: *the Deadly Triad*\n",
    "\n",
    "* Deep Reinforcement Learning and the Deadly Triad (Van Hasselt et al, 2018)\n",
    "\n",
    "> **Function approximation**: Una forma poderosa y escalable de generalizar a partir de un espacio de estados mucho más grande que la memoria y los recursos computacionales (por ejemplo, linear function approximation o ANN).\n",
    "\n",
    "> **Bootstrapping Update**: objetivos que incluyen estimaciones existentes (como en programación dinámica o métodos TD) en lugar de depender exclusivamente de recompensas reales y retornos completos (como en los métodos MC).\n",
    "\n",
    "> **Off-policy training**: Entrenamiento en una distribución de transiciones diferente a la producida por la política objetivo. Recorrer el espacio de estados y actualizar todos los estados de manera uniforme, no respeta la política objetivo y es un ejemplo de entrenamiento off-policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras al Deep Q-Learning\n",
    "\n",
    "![](images/deep_rl_deep_q_improvements.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](images/deep_rl_improvements_fixed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Human-level control through Deep-RL (Mnih et al. 2015)*\n",
    "\n",
    "![](images/deep_rl_deep_q_learning_er.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQNs\n",
    "\n",
    "Trata con el problema de sobreestimar algunos valores de Q. (Hado van Hasselt, 2010)\n",
    "\n",
    "* TD target: Cómo asegurar que la mejor acción para el estado siguiente es la acción con el mayor valor $Q$?\n",
    "* La exactitud de los valores $Q$ depende de que acciones se han ejecutado y que estados vecinos hemos explorado.\n",
    "\n",
    "* Al inicio del entrenamiento no tenemos suficiente información respecto de qué acciones elegir, por lo que tomar los mejores valores de $Q$ puede llevar a “falsos positivos” y a demorar la convergencia a largo plazo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_double_q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solución: Cuando calculamos el Q target, empleamos dos redes para desacoplar la selección de acciones de la generación del Q target.\n",
    "* La DQN Network selecciona cual es la mejor acción en el siguiente estado (acción con mayor Q).\n",
    "* La Target Network calcula el Q target correspondiente a tomar esa acción en el estado siguiente.\n",
    "\n",
    "> La idea de Double Q-learning es reducir sobreestimaciones descomponiendo la operación *max* en el target en **selección de acción y evaluación de acción**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_double_q_solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O, con parámetros...\n",
    "\n",
    "![](images/deep_rl_double_q_update.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay\n",
    "\n",
    "La idea central es que algunas experiencias pueden ser más importantes que otras para el entrenamiento, pero podrían ocurrir menos\n",
    "frecuentemente.\n",
    "\n",
    "* Como el batch de experiencia es sampleado uniformemente las experiencias importantes que ocurren raramente casi no tienen chances de\n",
    "ser elegidas.\n",
    "\n",
    "* Con PER, cambiamos la distribución del sampleo empleando un criterio para definir la prioridad de cada tupla de experiencia.\n",
    "\n",
    "* Le asignamos más prioridad a aquellas experiencias en donde existe una gran diferencia entre la predicción y el TD target, dado que esto implica que tenemos mucho para aprender acerca de dicha experiencia.\n",
    "\n",
    "* Para ello empleamos el valor absoluto del error TD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_per.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Problema: con el normal Experience Replay, usamos una regla de actualización estocástica.\n",
    "\n",
    "* Empleando un sampleo basado en prioridades, introducimos un sesgo en favor de los ejemplos con alta prioridad (más chances de\n",
    "ser elegidos), corriendo el riesgo de producir overfitting.\n",
    "\n",
    "* Para corregir el bias, empleamos Importance Sample Weights que ajustan la actualización reduciendo los pesos de los ejemplos vistos a\n",
    "menudo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_per_isw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_double_dqn_pp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN\n",
    "\n",
    "Q-values: cuan bueno es estar en el estado s y ejecutar la acción a (Q(s,a)).\n",
    "\n",
    "* Q(s,a) puede descomponerse como la suma de:\n",
    "* V(s): el valor de estar en el estado s.\n",
    "* A(s,a): la ventaja de elegir la acción a en dicho estado, o cuan mejor es seleccionar esa acción respecto de todas las posibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_dueling_advantage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con DDQN, separamos el estimador de los mencionados elementos empleando dos streams:\n",
    "\n",
    "* Uno que estima el valor del estado V(s).\n",
    "* Otro que estima la ventaja para cada acción A(s,a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_dueling_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente se combinan los streams en una capa de agregación para obtener la estimación de Q(s,a).\n",
    "\n",
    "* Por qué calculamos los valores separadamente y luego los combinamos?\n",
    "* Desacoplando la estimación la DDQN puede aprender cuáles estados son (o no) valorables sin tener que aprender el efecto de cada acción en cada estado (ya que se calcula $V(s)$ por separado!).\n",
    "* Con una DQN normal necesitamos calcular el valor de cada acción en cada estado. Si el estado es de por sí “malo” calcular el valor de las acciones no aporta al aprendizaje.\n",
    "* Al calcular $V(s)$, no es necesario calcular el valor de cada acción lo que es particularmente útil en aquellos estados en que no es relevante la acción que se toma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_dueling_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/deep_rl_dueling_q_value.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
